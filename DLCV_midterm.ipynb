{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midterm project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CITATIONS:\n",
    "- https://github.com/poojahira/gtsrb-pytorch\n",
    "- https://github.com/surajmurthy/TSR_PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "from torchvision.io import read_image\n",
    "from model import GTSRBnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize([212, 256]),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Define path of training data\n",
    "train_data_path = '/home/stringlab/Desktop/DLCV_midterm_project/GTSRB_Final_Training_Images/GTSRB/Final_Training/Images' \n",
    "train_data = torchvision.datasets.ImageFolder(root = train_data_path, transform=transform)\n",
    "\n",
    "# Divide data into training and validation set\n",
    "train_ratio = 0.9\n",
    "n_train_examples = int(len(train_data) * train_ratio)\n",
    "n_val_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, val_data = data.random_split(train_data, [n_train_examples, n_val_examples])\n",
    "print(f\"Number of training samples = {len(train_data)}\")\n",
    "print(f\"Number of validation samples = {len(val_data)}\")\n",
    "\n",
    "num_train_classes = len(train_data.dataset.classes)\n",
    "train_hist = [0]*num_train_classes\n",
    "for i in train_data.indices:\n",
    "    tar = train_data.dataset.targets[i]\n",
    "    train_hist[tar] += 1\n",
    "\n",
    "num_val_classes = len(val_data.dataset.classes)\n",
    "val_hist = [0]*num_val_classes\n",
    "for i in val_data.indices:\n",
    "    tar = val_data.dataset.targets[i]\n",
    "    val_hist[tar] += 1\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(num_train_classes), train_hist, label=\"train\")\n",
    "plt.bar(range(num_val_classes), val_hist, label=\"val\")\n",
    "#plt.bar(range(num_test_classes), test_hist, label=\"test\")\n",
    "legend = plt.legend(loc='upper right', shadow=True)\n",
    "plt.title(\"Distribution Plot\")\n",
    "plt.xlabel(\"Class ID\")\n",
    "plt.ylabel(\"# of examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader for training and validation\n",
    "BATCH_SIZE = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader = data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_loader = data.DataLoader(val_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Neural Network and Optimizer\n",
    "model = GTSRBnet(num_train_classes)\n",
    "model = model.to(device);\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    training_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        data, target = data.to(device), target.to(device)   \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        max_index = output.max(dim = 1)[1]\n",
    "        correct += (max_index == target).sum()\n",
    "        training_loss += loss\n",
    "    avg_train_loss = training_loss / len(train_loader.dataset)\n",
    "    avg_train_acc = 100. * correct / len(train_loader.dataset)\n",
    "    print('\\nTraining set: Average loss: {:.4f}, Accuracy: {:.0f}%\\n'.format(\n",
    "                avg_train_loss, avg_train_acc))\n",
    "    return avg_train_loss, avg_train_acc\n",
    "\n",
    "def validation():\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in val_loader:\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            validation_loss += F.nll_loss(output, target, size_average=False).data.item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    validation_loss /= len(val_loader.dataset)\n",
    "    scheduler.step(np.around(validation_loss,2))\n",
    "    validation_acc = 100. * correct / len(val_loader.dataset)\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {:.0f}%\\n'.format(\n",
    "        validation_loss, validation_acc))\n",
    "    return validation_loss, validation_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = [], []\n",
    "val_loss, val_acc = [], []\n",
    "for epoch in tqdm(range(1, 50)):\n",
    "    avg_train_loss, avg_train_acc = train(epoch)\n",
    "    avg_val_loss, avg_val_acc = validation()\n",
    "    train_loss.append(avg_train_loss)\n",
    "    train_acc.append(avg_train_acc)\n",
    "    val_loss.append(avg_val_loss)\n",
    "    val_acc.append(avg_val_acc)\n",
    "    model_file = 'trained_models/model_' + str(epoch) + '.pth'\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "    print('\\nSaved model to ' + model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(train_loss, label='train')\n",
    "ax[0].plot(val_loss, label='val')\n",
    "ax[0].set_title('Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(train_acc, label='train')\n",
    "ax[1].plot(val_acc, label='val')\n",
    "ax[1].set_title('Accuracy')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e019fd396ec95f049220eceb1ef17507a867cb3a5a0d714b39db56f962322af5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
