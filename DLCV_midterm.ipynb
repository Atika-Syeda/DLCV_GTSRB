{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midterm project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CITATIONS:\n",
    "- https://github.com/poojahira/gtsrb-pytorch\n",
    "- https://github.com/surajmurthy/TSR_PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, nclasses):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(3, 100, kernel_size=5)\n",
    "        self.bn1 = nn.BatchNorm2d(100)\n",
    "        self.conv2 = nn.Conv2d(100, 150, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(150)\n",
    "        self.conv3 = nn.Conv2d(150, 250, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(250)\n",
    "        self.conv_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(250*2*2, 350)\n",
    "        self.fc2 = nn.Linear(350, nclasses)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "            )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 4 * 4, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "            )\n",
    "   \n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 4 * 4)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transform the input\n",
    "        x = self.stn(x)\n",
    "\n",
    "        # Perform forward pass\n",
    "        x = self.bn1(F.max_pool2d(F.leaky_relu(self.conv1(x)),2))\n",
    "        x = self.conv_drop(x)\n",
    "        x = self.bn2(F.max_pool2d(F.leaky_relu(self.conv2(x)),2))\n",
    "        x = self.conv_drop(x)\n",
    "        x = self.bn3(F.max_pool2d(F.leaky_relu(self.conv3(x)),2))\n",
    "        x = self.conv_drop(x)\n",
    "        x = x.view(-1, 250*2*2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "class FMnet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes,\n",
    "        img_ch=1,\n",
    "        channels=[16, 32, 64, 128, 200],\n",
    "        device=['cuda' if torch.cuda.is_available() else 'cpu'][0],\n",
    "        kernel=3,\n",
    "        shape=(256, 256),\n",
    "        n_upsample=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_upsample = n_upsample\n",
    "        self.image_shape = shape\n",
    "        self.device = device\n",
    "        self.channels = channels\n",
    "\n",
    "        self.Conv = nn.Sequential()\n",
    "        self.Conv.add_module(\n",
    "            \"conv0\",\n",
    "            convblock(ch_in=img_ch, ch_out=channels[0], kernel_sz=kernel, block=0),\n",
    "        )\n",
    "        for k in range(1, len(channels)):\n",
    "            self.Conv.add_module(\n",
    "                f\"conv{k}\",\n",
    "                convblock(\n",
    "                    ch_in=channels[k - 1], ch_out=channels[k], kernel_sz=kernel, block=k\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.conv_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(self.channels[-1]*14*16, 350)\n",
    "        self.fc2 = nn.Linear(350, n_classes)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "            )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 4 * 4, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "            )\n",
    "   \n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def forward(self, x, normalize=False, verbose=False):\n",
    "        # encoding path\n",
    "        xout = []\n",
    "        x = self.Conv[0](x)\n",
    "        xout.append(x)\n",
    "        for k in range(1, len(self.Conv)):\n",
    "            x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
    "            x = self.Conv[k](x)\n",
    "            xout.append(x)\n",
    "\n",
    "        # transform the input\n",
    "        x = self.conv_drop(x)\n",
    "        x = x.view(-1, self.channels[-1]*14*16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class convblock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, kernel_sz, block=-1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential()\n",
    "        self.block = block\n",
    "        if self.block != 0:\n",
    "            self.conv.add_module(\"conv_0\", batchconv(ch_in, ch_out, kernel_sz))\n",
    "        else:\n",
    "            self.conv.add_module(\"conv_0\", batchconv0(ch_in, ch_out, kernel_sz))\n",
    "        self.conv.add_module(\"conv_1\", batchconv(ch_out, ch_out, kernel_sz))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv[1](self.conv[0](x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def batchconv0(ch_in, ch_out, kernel_sz):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(ch_in, eps=1e-5, momentum=0.1),\n",
    "        nn.Conv2d(ch_in, ch_out, kernel_sz, padding=kernel_sz // 2, bias=False),\n",
    "    )\n",
    "\n",
    "\n",
    "def batchconv(ch_in, ch_out, sz):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(ch_in, eps=1e-5, momentum=0.1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(ch_in, ch_out, sz, padding=sz // 2, bias=False),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix test data targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples = 35288\n",
      "Number of validation samples = 3921\n"
     ]
    }
   ],
   "source": [
    "# Define path of training data\n",
    "transform = transforms.Compose([\n",
    "    # you can add other transformations in this list\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize([212, 256]),\n",
    "    transforms.ToTensor()\n",
    "    #transforms.Resize((32, 32)),\n",
    "    #transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629))\n",
    "])\n",
    "\n",
    "train_data_path = '/home/stringlab/Desktop/DLCV_midterm_project/GTSRB_Final_Training_Images/GTSRB/Final_Training/Images' \n",
    "train_data = torchvision.datasets.ImageFolder(root = train_data_path, transform=transform)\n",
    "\n",
    "test_data_path = '/home/stringlab/Desktop/DLCV_midterm_project/GTSRB_Final_Test_Images/GTSRB'\n",
    "test_data = torchvision.datasets.ImageFolder(root = test_data_path, transform=transform)\n",
    "\n",
    "# Divide data into training and validation \n",
    "ratio = 0.9\n",
    "n_train_examples = int(len(train_data) * ratio)\n",
    "n_val_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, val_data = data.random_split(train_data, [n_train_examples, n_val_examples])\n",
    "print(f\"Number of training samples = {len(train_data)}\")\n",
    "print(f\"Number of validation samples = {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_classes = len(train_data.dataset.classes)\n",
    "train_hist = [0]*num_train_classes\n",
    "for i in train_data.indices:\n",
    "    tar = train_data.dataset.targets[i]\n",
    "    train_hist[tar] += 1\n",
    "\n",
    "num_val_classes = len(val_data.dataset.classes)\n",
    "val_hist = [0]*num_val_classes\n",
    "for i in val_data.indices:\n",
    "    tar = val_data.dataset.targets[i]\n",
    "    val_hist[tar] += 1\n",
    "\n",
    "num_test_classes = len(np.unique(test_data.targets))\n",
    "test_hist = [0]*num_test_classes\n",
    "for i, t in enumerate(test_data.targets):\n",
    "    test_hist[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '# of examples')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(num_train_classes), train_hist, label=\"train\")\n",
    "plt.bar(range(num_val_classes), val_hist, label=\"val\")\n",
    "#plt.bar(range(num_test_classes), test_hist, label=\"test\")\n",
    "legend = plt.legend(loc='upper right', shadow=True)\n",
    "plt.title(\"Distribution Plot\")\n",
    "plt.xlabel(\"Class ID\")\n",
    "plt.ylabel(\"# of examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader for training and validation\n",
    "BATCH_SIZE = 100\n",
    "LOG_INTERVAL = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader = data.DataLoader(train_data, shuffle=True, batch_size = BATCH_SIZE)\n",
    "val_loader = data.DataLoader(val_data, shuffle=True, batch_size = BATCH_SIZE)\n",
    "test_loader = data.DataLoader(test_data, shuffle=True, batch_size = BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FMnet(\n",
       "  (Conv): Sequential(\n",
       "    (conv0): convblock(\n",
       "      (conv): Sequential(\n",
       "        (conv_0): Sequential(\n",
       "          (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv1): convblock(\n",
       "      (conv): Sequential(\n",
       "        (conv_0): Sequential(\n",
       "          (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv2): convblock(\n",
       "      (conv): Sequential(\n",
       "        (conv_0): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv3): convblock(\n",
       "      (conv): Sequential(\n",
       "        (conv_0): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv4): convblock(\n",
       "      (conv): Sequential(\n",
       "        (conv_0): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(128, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=44800, out_features=350, bias=True)\n",
       "  (fc2): Linear(in_features=350, out_features=43, bias=True)\n",
       "  (localization): Sequential(\n",
       "    (0): Conv2d(3, 8, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc_loc): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neural Network and Optimizer\n",
    "model = FMnet(num_train_classes)#Net(nclasses=43)#FMnet(num_train_classes)\n",
    "model = model.to(device);\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    training_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        data, target = data.to(device), target.to(device)   \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        max_index = output.max(dim = 1)[1]\n",
    "        correct += (max_index == target).sum()\n",
    "        training_loss += loss\n",
    "        if batch_idx % 2 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss per example: {:.6f}\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), loss.data.item()/(BATCH_SIZE * LOG_INTERVAL),loss.data.item()))\n",
    "    print('\\nTraining set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                training_loss / len(train_loader.dataset), correct, len(train_loader.dataset),\n",
    "                100. * correct / len(train_loader.dataset)))\n",
    "\n",
    "def validation():\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in val_loader:\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            validation_loss += F.nll_loss(output, target, size_average=False).data.item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    validation_loss /= len(val_loader.dataset)\n",
    "    scheduler.step(np.around(validation_loss,2))\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        validation_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/35288 (0%)]\tLoss per example: 0.000378\tLoss: 3.777741\n",
      "Train Epoch: 1 [200/35288 (1%)]\tLoss per example: 0.000676\tLoss: 6.763307\n",
      "Train Epoch: 1 [400/35288 (1%)]\tLoss per example: 0.000488\tLoss: 4.880466\n",
      "Train Epoch: 1 [600/35288 (2%)]\tLoss per example: 0.000505\tLoss: 5.049462\n",
      "Train Epoch: 1 [800/35288 (2%)]\tLoss per example: 0.000439\tLoss: 4.387559\n",
      "Train Epoch: 1 [1000/35288 (3%)]\tLoss per example: 0.000515\tLoss: 5.152693\n",
      "Train Epoch: 1 [1200/35288 (3%)]\tLoss per example: 0.000391\tLoss: 3.907191\n",
      "Train Epoch: 1 [1400/35288 (4%)]\tLoss per example: 0.000392\tLoss: 3.917132\n",
      "Train Epoch: 1 [1600/35288 (5%)]\tLoss per example: 0.000423\tLoss: 4.231739\n",
      "Train Epoch: 1 [1800/35288 (5%)]\tLoss per example: 0.000417\tLoss: 4.171200\n",
      "Train Epoch: 1 [2000/35288 (6%)]\tLoss per example: 0.000383\tLoss: 3.826601\n",
      "Train Epoch: 1 [2200/35288 (6%)]\tLoss per example: 0.000377\tLoss: 3.769854\n",
      "Train Epoch: 1 [2400/35288 (7%)]\tLoss per example: 0.000370\tLoss: 3.704746\n",
      "Train Epoch: 1 [2600/35288 (7%)]\tLoss per example: 0.000380\tLoss: 3.797591\n",
      "Train Epoch: 1 [2800/35288 (8%)]\tLoss per example: 0.000377\tLoss: 3.766867\n",
      "Train Epoch: 1 [3000/35288 (8%)]\tLoss per example: 0.000397\tLoss: 3.974827\n",
      "Train Epoch: 1 [3200/35288 (9%)]\tLoss per example: 0.000370\tLoss: 3.700366\n",
      "Train Epoch: 1 [3400/35288 (10%)]\tLoss per example: 0.000367\tLoss: 3.667737\n",
      "Train Epoch: 1 [3600/35288 (10%)]\tLoss per example: 0.000362\tLoss: 3.623130\n",
      "Train Epoch: 1 [3800/35288 (11%)]\tLoss per example: 0.000371\tLoss: 3.707800\n",
      "Train Epoch: 1 [4000/35288 (11%)]\tLoss per example: 0.000359\tLoss: 3.588340\n",
      "Train Epoch: 1 [4200/35288 (12%)]\tLoss per example: 0.000390\tLoss: 3.895916\n",
      "Train Epoch: 1 [4400/35288 (12%)]\tLoss per example: 0.000374\tLoss: 3.739375\n",
      "Train Epoch: 1 [4600/35288 (13%)]\tLoss per example: 0.000362\tLoss: 3.621033\n",
      "Train Epoch: 1 [4800/35288 (14%)]\tLoss per example: 0.000363\tLoss: 3.627478\n",
      "Train Epoch: 1 [5000/35288 (14%)]\tLoss per example: 0.000361\tLoss: 3.609199\n",
      "Train Epoch: 1 [5200/35288 (15%)]\tLoss per example: 0.000362\tLoss: 3.623869\n",
      "Train Epoch: 1 [5400/35288 (15%)]\tLoss per example: 0.000365\tLoss: 3.645739\n",
      "Train Epoch: 1 [5600/35288 (16%)]\tLoss per example: 0.000375\tLoss: 3.751130\n",
      "Train Epoch: 1 [5800/35288 (16%)]\tLoss per example: 0.000378\tLoss: 3.775737\n",
      "Train Epoch: 1 [6000/35288 (17%)]\tLoss per example: 0.000371\tLoss: 3.713488\n",
      "Train Epoch: 1 [6200/35288 (18%)]\tLoss per example: 0.000355\tLoss: 3.554298\n",
      "Train Epoch: 1 [6400/35288 (18%)]\tLoss per example: 0.000361\tLoss: 3.613497\n",
      "Train Epoch: 1 [6600/35288 (19%)]\tLoss per example: 0.000357\tLoss: 3.565888\n",
      "Train Epoch: 1 [6800/35288 (19%)]\tLoss per example: 0.000367\tLoss: 3.665834\n",
      "Train Epoch: 1 [7000/35288 (20%)]\tLoss per example: 0.000365\tLoss: 3.652266\n",
      "Train Epoch: 1 [7200/35288 (20%)]\tLoss per example: 0.000371\tLoss: 3.714616\n",
      "Train Epoch: 1 [7400/35288 (21%)]\tLoss per example: 0.000361\tLoss: 3.607059\n",
      "Train Epoch: 1 [7600/35288 (22%)]\tLoss per example: 0.000363\tLoss: 3.629916\n",
      "Train Epoch: 1 [7800/35288 (22%)]\tLoss per example: 0.000357\tLoss: 3.569928\n",
      "Train Epoch: 1 [8000/35288 (23%)]\tLoss per example: 0.000367\tLoss: 3.669456\n",
      "Train Epoch: 1 [8200/35288 (23%)]\tLoss per example: 0.000373\tLoss: 3.732251\n",
      "Train Epoch: 1 [8400/35288 (24%)]\tLoss per example: 0.000366\tLoss: 3.655110\n",
      "Train Epoch: 1 [8600/35288 (24%)]\tLoss per example: 0.000367\tLoss: 3.667847\n",
      "Train Epoch: 1 [8800/35288 (25%)]\tLoss per example: 0.000363\tLoss: 3.627054\n",
      "Train Epoch: 1 [9000/35288 (25%)]\tLoss per example: 0.000360\tLoss: 3.603619\n",
      "Train Epoch: 1 [9200/35288 (26%)]\tLoss per example: 0.000373\tLoss: 3.734604\n",
      "Train Epoch: 1 [9400/35288 (27%)]\tLoss per example: 0.000364\tLoss: 3.642332\n",
      "Train Epoch: 1 [9600/35288 (27%)]\tLoss per example: 0.000357\tLoss: 3.568996\n",
      "Train Epoch: 1 [9800/35288 (28%)]\tLoss per example: 0.000365\tLoss: 3.650382\n",
      "Train Epoch: 1 [10000/35288 (28%)]\tLoss per example: 0.000362\tLoss: 3.616126\n",
      "Train Epoch: 1 [10200/35288 (29%)]\tLoss per example: 0.000371\tLoss: 3.712437\n",
      "Train Epoch: 1 [10400/35288 (29%)]\tLoss per example: 0.000364\tLoss: 3.643412\n",
      "Train Epoch: 1 [10600/35288 (30%)]\tLoss per example: 0.000379\tLoss: 3.793356\n",
      "Train Epoch: 1 [10800/35288 (31%)]\tLoss per example: 0.000361\tLoss: 3.609861\n",
      "Train Epoch: 1 [11000/35288 (31%)]\tLoss per example: 0.000354\tLoss: 3.537482\n",
      "Train Epoch: 1 [11200/35288 (32%)]\tLoss per example: 0.000352\tLoss: 3.522128\n",
      "Train Epoch: 1 [11400/35288 (32%)]\tLoss per example: 0.000362\tLoss: 3.618464\n",
      "Train Epoch: 1 [11600/35288 (33%)]\tLoss per example: 0.000365\tLoss: 3.645359\n",
      "Train Epoch: 1 [11800/35288 (33%)]\tLoss per example: 0.000365\tLoss: 3.652714\n",
      "Train Epoch: 1 [12000/35288 (34%)]\tLoss per example: 0.000355\tLoss: 3.549835\n",
      "Train Epoch: 1 [12200/35288 (35%)]\tLoss per example: 0.000355\tLoss: 3.551796\n",
      "Train Epoch: 1 [12400/35288 (35%)]\tLoss per example: 0.000374\tLoss: 3.743036\n",
      "Train Epoch: 1 [12600/35288 (36%)]\tLoss per example: 0.000360\tLoss: 3.603804\n",
      "Train Epoch: 1 [12800/35288 (36%)]\tLoss per example: 0.000360\tLoss: 3.600068\n",
      "Train Epoch: 1 [13000/35288 (37%)]\tLoss per example: 0.000359\tLoss: 3.593100\n",
      "Train Epoch: 1 [13200/35288 (37%)]\tLoss per example: 0.000360\tLoss: 3.597430\n",
      "Train Epoch: 1 [13400/35288 (38%)]\tLoss per example: 0.000354\tLoss: 3.538821\n",
      "Train Epoch: 1 [13600/35288 (39%)]\tLoss per example: 0.000356\tLoss: 3.555403\n",
      "Train Epoch: 1 [13800/35288 (39%)]\tLoss per example: 0.000362\tLoss: 3.619632\n",
      "Train Epoch: 1 [14000/35288 (40%)]\tLoss per example: 0.000347\tLoss: 3.471971\n",
      "Train Epoch: 1 [14200/35288 (40%)]\tLoss per example: 0.000355\tLoss: 3.546164\n",
      "Train Epoch: 1 [14400/35288 (41%)]\tLoss per example: 0.000361\tLoss: 3.605374\n",
      "Train Epoch: 1 [14600/35288 (41%)]\tLoss per example: 0.000350\tLoss: 3.500829\n",
      "Train Epoch: 1 [14800/35288 (42%)]\tLoss per example: 0.000358\tLoss: 3.583694\n",
      "Train Epoch: 1 [15000/35288 (42%)]\tLoss per example: 0.000355\tLoss: 3.546036\n",
      "Train Epoch: 1 [15200/35288 (43%)]\tLoss per example: 0.000355\tLoss: 3.548197\n",
      "Train Epoch: 1 [15400/35288 (44%)]\tLoss per example: 0.000359\tLoss: 3.593152\n",
      "Train Epoch: 1 [15600/35288 (44%)]\tLoss per example: 0.000354\tLoss: 3.535829\n",
      "Train Epoch: 1 [15800/35288 (45%)]\tLoss per example: 0.000349\tLoss: 3.489514\n",
      "Train Epoch: 1 [16000/35288 (45%)]\tLoss per example: 0.000366\tLoss: 3.657597\n",
      "Train Epoch: 1 [16200/35288 (46%)]\tLoss per example: 0.000365\tLoss: 3.646297\n",
      "Train Epoch: 1 [16400/35288 (46%)]\tLoss per example: 0.000361\tLoss: 3.606819\n",
      "Train Epoch: 1 [16600/35288 (47%)]\tLoss per example: 0.000362\tLoss: 3.621197\n",
      "Train Epoch: 1 [16800/35288 (48%)]\tLoss per example: 0.000350\tLoss: 3.504788\n",
      "Train Epoch: 1 [17000/35288 (48%)]\tLoss per example: 0.000347\tLoss: 3.474467\n",
      "Train Epoch: 1 [17200/35288 (49%)]\tLoss per example: 0.000353\tLoss: 3.533913\n",
      "Train Epoch: 1 [17400/35288 (49%)]\tLoss per example: 0.000350\tLoss: 3.497569\n",
      "Train Epoch: 1 [17600/35288 (50%)]\tLoss per example: 0.000346\tLoss: 3.457306\n",
      "Train Epoch: 1 [17800/35288 (50%)]\tLoss per example: 0.000346\tLoss: 3.458271\n",
      "Train Epoch: 1 [18000/35288 (51%)]\tLoss per example: 0.000360\tLoss: 3.595109\n",
      "Train Epoch: 1 [18200/35288 (52%)]\tLoss per example: 0.000354\tLoss: 3.544234\n",
      "Train Epoch: 1 [18400/35288 (52%)]\tLoss per example: 0.000359\tLoss: 3.594217\n",
      "Train Epoch: 1 [18600/35288 (53%)]\tLoss per example: 0.000346\tLoss: 3.462546\n",
      "Train Epoch: 1 [18800/35288 (53%)]\tLoss per example: 0.000346\tLoss: 3.456750\n",
      "Train Epoch: 1 [19000/35288 (54%)]\tLoss per example: 0.000339\tLoss: 3.392831\n",
      "Train Epoch: 1 [19200/35288 (54%)]\tLoss per example: 0.000342\tLoss: 3.420088\n",
      "Train Epoch: 1 [19400/35288 (55%)]\tLoss per example: 0.000344\tLoss: 3.441930\n",
      "Train Epoch: 1 [19600/35288 (56%)]\tLoss per example: 0.000329\tLoss: 3.288587\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 50):\n",
    "    train(epoch)\n",
    "    validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e019fd396ec95f049220eceb1ef17507a867cb3a5a0d714b39db56f962322af5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
